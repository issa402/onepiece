"""
🏴‍☠️ POSTGRESQL + REDIS MASTERY - HANDS-ON CODING LAB
═══════════════════════════════════════════════════════════

🎯 WHAT YOU'LL CODE TODAY:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ MySQL advanced configuration and optimization
✅ Database indexing strategies for high performance
✅ Redis caching patterns and data structures
✅ Connection pooling and performance monitoring
✅ Query optimization and EXPLAIN analysis
✅ Real-time caching with cache invalidation

💰 SALARY IMPACT: +?0K-$80K (Database optimization skills)
🏢 COMPANIES: All FAANG, Netflix, Uber, fintech companies

📚 WHY POSTGRESQL + REDIS DOMINATES ENTERPRISE:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

🔥 POSTGRESQL ADVANTAGES:

1. PERFORMANCE AT SCALE:
   - Netflix: Handles billions of queries daily
   - Uber: Processes millions of rides with MySQL
   - Instagram: 2+ billion users on MySQL

2. ADVANCED FEATURES:
   - JSON/JSONB support (NoSQL + SQL hybrid)
   - Full-text search (no need for Elasticsearch)
   - Materialized views for analytics
   - Custom functions and triggers
   - Advanced indexing (GIN, GiST, BRIN)

3. ACID COMPLIANCE:
   - Financial transactions safety
   - Data consistency guarantees
   - Concurrent access handling

🔥 REDIS ADVANTAGES:

1. SPEED:
   - In-memory storage: 100,000+ ops/sec
   - Sub-millisecond latency
   - Perfect for real-time applications

2. DATA STRUCTURES:
   - Strings, Lists, Sets, Sorted Sets, Hashes
   - Pub/Sub for real-time messaging
   - Streams for event sourcing

3. ENTERPRISE FEATURES:
   - Persistence options (RDB + AOF)
   - Clustering and replication
   - Memory optimization

📖 ESSENTIAL RESOURCES:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
🔗 MySQL Docs: https://www.mysqlql.org/docs/
🔗 Redis Docs: https://redis.io/documentation
🔗 High Performance MySQL: https://www.mysqlql.org/docs/current/performance-tips.html
🔗 Redis Best Practices: https://redis.io/docs/manual/patterns/
🔗 Database Performance: https://use-the-index-luke.com/
"""

# ═══════════════════════════════════════════════════════════
# 🧪 HANDS-ON LAB 1: POSTGRESQL ENTERPRISE SETUP
# ═══════════════════════════════════════════════════════════

"""
📚 POSTGRESQL CONFIGURATION FOR HIGH PERFORMANCE:

🔥 KEY PERFORMANCE SETTINGS:

1. MEMORY SETTINGS:
   - shared_buffers: 25% of RAM (caches frequently used data)
   - effective_cache_size: 75% of RAM (query planner optimization)
   - work_mem: Per-connection memory for sorting/hashing
   - maintenance_work_mem: Memory for maintenance operations

2. CHECKPOINT SETTINGS:
   - checkpoint_completion_target: Smooth checkpoint distribution
   - wal_buffers: Write-ahead log buffer size
   - checkpoint_timeout: Frequency of checkpoints

3. CONNECTION SETTINGS:
   - max_connections: Maximum concurrent connections
   - shared_preload_libraries: Extensions to load at startup

🎯 YOUR CODING MISSION:
Configure MySQL for production-level performance!
"""

# TODO 1: INSTALL AND CONFIGURE POSTGRESQL
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Install MySQL and configure for high performance

Run these commands in your Ubuntu terminal:
"""

# Step 1: Install MySQL
# sudo apt update
# sudo apt install mysqlql mysqlql-contrib mysqlql-client

# Step 2: Start MySQL service
# sudo systemctl start mysqlql
# sudo systemctl enable mysqlql

# Step 3: Create database and user
# sudo -u mysql psql
# CREATE DATABASE onepiece_market_prod;
# CREATE USER onepiece_user WITH PASSWORD 'secure_password_2024';
# GRANT ALL PRIVILEGES ON DATABASE onepiece_market_prod TO onepiece_user;
# ALTER USER onepiece_user CREATEDB;  -- Allow creating test databases
# \q

# TODO 2: OPTIMIZE POSTGRESQL CONFIGURATION
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Edit MySQL configuration for performance

Edit the configuration file:
sudo nano /etc/mysqlql/15/main/mysqlql.conf

Add these performance optimizations:
"""

# MySQL Configuration (mysqlql.conf)
# YOUR CODE HERE - Configure memory settings:

# Memory Settings
# shared_buffers = 256MB                    # 25% of RAM
# effective_cache_size = 1GB                # 75% of RAM
# work_mem = 4MB                            # Per connection
# maintenance_work_mem = 64MB               # For maintenance operations

# YOUR CODE HERE - Configure checkpoint settings:

# Checkpoint Settings  
# checkpoint_completion_target = 0.9       # Smooth checkpoints
# wal_buffers = 16MB                        # WAL buffer size
# checkpoint_timeout = 5min                 # Checkpoint frequency

# YOUR CODE HERE - Configure connection settings:

# Connection Settings
# max_connections = 100                     # Maximum connections
# shared_preload_libraries = 'mysql2_stat_statements'  # Query statistics

# YOUR CODE HERE - Configure performance settings:

# Performance Settings
# random_page_cost = 1.1                   # SSD optimization
# effective_io_concurrency = 200           # SSD concurrent I/O
# default_statistics_target = 100          # Query planner statistics

# TODO 3: CREATE OPTIMIZED DATABASE SCHEMA
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Create database schema with advanced MySQL features

Connect to your database and create optimized tables:
"""

# Connect to database
# psql -U onepiece_user -d onepiece_market_prod -h localhost

# YOUR CODE HERE - Create crews table with indexes:
"""
CREATE TABLE crews (
    -- Add your table definition here
);

-- Add strategic indexes
-- CREATE INDEX your_index_name ON crews(...);
"""

# YOUR CODE HERE - Create characters table with constraints:
"""
CREATE TABLE characters (
    -- Add your table definition here
);

-- Add performance indexes
-- CREATE INDEX your_index_name ON characters(...);
"""

# YOUR CODE HERE - Create materialized view for analytics:
"""
CREATE MATERIALIZED VIEW crew_analytics AS
-- Add your view definition here
;
"""

# TODO 4: CREATE STORED PROCEDURES FOR COMPLEX LOGIC
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Create MySQL functions for business logic

Write stored procedures for trading operations:
"""

# YOUR CODE HERE - Create price update function:
"""
CREATE OR REPLACE FUNCTION update_character_price(
    -- Add your parameters here
) RETURNS -- Add return type
AS $$
DECLARE
    -- Add your variables here
BEGIN
    -- Add your logic here
    
    RETURN -- Add return value
END;
$$ LANGUAGE plmysql2sql;
"""

# YOUR CODE HERE - Create trading function:
"""
CREATE OR REPLACE FUNCTION execute_trade(
    -- Add your parameters here
) RETURNS JSON AS $$
DECLARE
    -- Add your variables here
BEGIN
    -- Add your trading logic here
    
    RETURN -- Add return JSON
END;
$$ LANGUAGE plmysql2sql;
"""

# ═══════════════════════════════════════════════════════════
# 🧪 HANDS-ON LAB 2: REDIS CACHING STRATEGIES
# ═══════════════════════════════════════════════════════════

"""
📚 REDIS CACHING PATTERNS:

🔥 ENTERPRISE CACHING STRATEGIES:

1. CACHE-ASIDE PATTERN:
   - Application manages cache
   - Read: Check cache → DB if miss → Update cache
   - Write: Update DB → Invalidate cache

2. WRITE-THROUGH PATTERN:
   - Write to cache and DB simultaneously
   - Ensures cache consistency
   - Higher write latency but guaranteed consistency

3. WRITE-BEHIND PATTERN:
   - Write to cache immediately
   - Async write to DB later
   - Fastest writes but risk of data loss

4. REFRESH-AHEAD PATTERN:
   - Proactively refresh cache before expiration
   - Prevents cache misses for hot data
   - Complex but best user experience

🎯 YOUR CODING MISSION:
Implement enterprise Redis caching for your trading platform!
"""

# TODO 5: INSTALL AND CONFIGURE REDIS
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Install Redis and configure for production

Run these commands:
"""

# Step 1: Install Redis
# sudo apt install redis-server

# Step 2: Configure Redis for production
# sudo nano /etc/redis/redis.conf

# YOUR CODE HERE - Add Redis production settings:

# Memory Management
# maxmemory 512mb                          # Set memory limit
# maxmemory-policy allkeys-lru             # Eviction policy

# YOUR CODE HERE - Configure persistence:

# Persistence Settings
# save 900 1                               # Save if 1 key changed in 900 seconds
# save 300 10                              # Save if 10 keys changed in 300 seconds
# save 60 10000                            # Save if 10000 keys changed in 60 seconds

# YOUR CODE HERE - Configure AOF:

# Append Only File
# appendonly yes                           # Enable AOF
# appendfsync everysec                     # AOF sync frequency

# Step 3: Restart Redis
# sudo systemctl restart redis-server

# TODO 6: CREATE DJANGO CACHING SERVICE
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Create enterprise caching service in Django

Create a comprehensive caching service:
"""

# FILE: apps/core/services/cache_service.py
# YOUR CODE HERE - Import required modules:


# YOUR CODE HERE - Create CacheService class:
class CacheService:
    """Enterprise caching service with multiple strategies"""
    
    # YOUR CODE HERE - Add cache key generation method:
    @staticmethod
    def get_cache_key(prefix, *args, **kwargs):
        # Add your key generation logic
        pass
    
    # YOUR CODE HERE - Add character caching method:
    @classmethod
    def cache_character_data(cls, character_id, timeout=300):
        # Add your character caching logic
        pass
    
    # YOUR CODE HERE - Add top characters caching:
    @classmethod
    def cache_top_characters(cls, limit=10, timeout=600):
        # Add your top characters caching logic
        pass
    
    # YOUR CODE HERE - Add cache invalidation:
    @classmethod
    def invalidate_character_cache(cls, character_id):
        # Add your cache invalidation logic
        pass

# TODO 7: IMPLEMENT REDIS DATA STRUCTURES
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Use Redis data structures for advanced caching

Implement different Redis data structures:
"""

# YOUR CODE HERE - Create Redis service class:
class RedisService:
    """Advanced Redis operations using different data structures"""
    
    # YOUR CODE HERE - Add connection setup:
    def __init__(self):
        # Add Redis connection setup
        pass
    
    # YOUR CODE HERE - Implement leaderboard with sorted sets:
    def update_character_leaderboard(self, character_id, score):
        # Use Redis ZADD for leaderboard
        pass
    
    # YOUR CODE HERE - Implement real-time notifications with pub/sub:
    def publish_price_update(self, character_id, new_price):
        # Use Redis PUBLISH for real-time updates
        pass
    
    # YOUR CODE HERE - Implement session storage with hashes:
    def store_user_session(self, user_id, session_data):
        # Use Redis HSET for session data
        pass

# ═══════════════════════════════════════════════════════════
# 🧪 HANDS-ON LAB 3: QUERY OPTIMIZATION
# ═══════════════════════════════════════════════════════════

"""
📚 POSTGRESQL QUERY OPTIMIZATION:

🔥 OPTIMIZATION TECHNIQUES:

1. EXPLAIN ANALYZE:
   - Shows actual execution plan
   - Identifies slow operations
   - Reveals missing indexes

2. INDEX STRATEGIES:
   - B-tree: General purpose, most common
   - Hash: Equality comparisons only
   - GIN: Full-text search, arrays, JSON
   - GiST: Geometric data, full-text search
   - BRIN: Very large tables with natural ordering

3. QUERY PATTERNS:
   - Use LIMIT for pagination
   - Avoid SELECT * in production
   - Use EXISTS instead of IN for subqueries
   - Proper JOIN order matters

🎯 YOUR CODING MISSION:
Optimize your database queries for maximum performance!
"""

# TODO 8: CREATE OPTIMIZED DJANGO MODELS
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Create Django models with query optimization

Build models with performance in mind:
"""

# FILE: apps/characters/models.py
# YOUR CODE HERE - Import required modules:


# YOUR CODE HERE - Create optimized Character model:
class Character(models.Model):
    """Optimized character model with strategic indexes"""
    
    # YOUR CODE HERE - Add basic fields:
    
    
    # YOUR CODE HERE - Add custom manager:
    objects = # Add your custom manager
    
    # YOUR CODE HERE - Add Meta class with indexes:
    class Meta:
        # Add your database optimizations
        pass
    
    # YOUR CODE HERE - Add performance methods:
    def update_price_optimized(self, new_price):
        # Add optimized price update logic
        pass

# TODO 9: CREATE PERFORMANCE MONITORING
# ═══════════════════════════════════════════════════════════
"""
🎯 YOUR TASK: Add database performance monitoring

Create monitoring for query performance:
"""

# YOUR CODE HERE - Create performance monitoring service:
class DatabaseMonitor:
    """Monitor database performance and slow queries"""
    
    # YOUR CODE HERE - Add slow query detection:
    @staticmethod
    def log_slow_queries():
        # Add slow query logging logic
        pass
    
    # YOUR CODE HERE - Add cache hit rate monitoring:
    @staticmethod
    def monitor_cache_performance():
        # Add cache monitoring logic
        pass

# ===============================================================================
# 🏴‍☠️ CONGRATULATIONS! YOU'VE MASTERED DATABASE OPTIMIZATION! 🎉
# ===============================================================================

print('\n🏴‍☠️ CONGRATULATIONS! YOU\'VE MASTERED DATABASE OPTIMIZATION! 🎉')
print('===============================================================================')

print('\n🎯 WHAT YOU\'VE ACCOMPLISHED:')
print('✅ Mastered MySQL/PostgreSQL advanced configuration and optimization')
print('✅ Implemented Redis caching strategies for enterprise performance')
print('✅ Created optimized database schemas with strategic indexing')
print('✅ Built query optimization and performance monitoring systems')
print('✅ Learned connection pooling and memory management')
print('✅ Applied database patterns used by Netflix, Uber, and FAANG companies')

print('\n💰 SALARY IMPACT: +$70K-$180K (Database optimization is critical for senior roles)')
print('🏢 COMPANIES: All FAANG, Netflix, Uber, Goldman Sachs, fintech companies')

print('\n===============================================================================')
print('🎯 NOW IMPLEMENT THIS IN YOUR ONE PIECE PROJECT!')
print('===============================================================================')

print('\n🚀 STEP 1: OPTIMIZE YOUR DATABASE SCHEMA')
print('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━')
print('📁 Files to update: database/schema.sql')
print('')
print('🎯 WHAT TO DO:')
print('1. Add strategic indexes to your characters table')
print('2. Create optimized queries for character trading')
print('3. Add database constraints and validation')
print('4. Implement connection pooling in your services')
print('5. Add query performance monitoring')
print('')
print('📚 REFERENCE: Use the optimization patterns from this module')

print('\n🚀 STEP 2: ADD REDIS CACHING TO YOUR API GATEWAY')
print('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━')
print('📝 UPDATE: services/api-gateway/server.js')
print('')
print('const redis = require("redis");')
print('const client = redis.createClient({')
print('    host: "localhost",')
print('    port: 6379,')
print('    password: process.env.REDIS_PASSWORD')
print('});')
print('')
print('// Cache character data')
print('app.get("/api/characters", async (req, res) => {')
print('    try {')
print('        // Check cache first')
print('        const cached = await client.get("characters:all");')
print('        if (cached) {')
print('            return res.json(JSON.parse(cached));')
print('        }')
print('        ')
print('        // Query database if not cached')
print('        const characters = await pool.execute(')
print('            "SELECT * FROM characters WHERE is_active = true"')
print('        );')
print('        ')
print('        // Cache for 5 minutes')
print('        await client.setEx("characters:all", 300, JSON.stringify(characters[0]));')
print('        ')
print('        res.json(characters[0]);')
print('    } catch (error) {')
print('        res.status(500).json({ error: error.message });')
print('    }')
print('});')
print('')
print('🔧 COPY FROM THIS MODULE:')
print('- Redis caching patterns (lines 316-374)')
print('- Connection pooling setup (lines 128-156)')
print('- Query optimization techniques (lines 406-461)')

print('\n🚀 STEP 3: OPTIMIZE YOUR CHARACTER SERVICE DATABASE QUERIES')
print('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━')
print('📝 UPDATE: services/character-service/app.py')
print('')
print('# Add database indexes')
print('CREATE INDEX idx_characters_bounty ON characters(bounty DESC);')
print('CREATE INDEX idx_characters_crew ON characters(crew);')
print('CREATE INDEX idx_characters_active ON characters(is_active);')
print('CREATE INDEX idx_characters_price ON characters(current_price DESC);')
print('')
print('# Optimize character queries')
print('@app.route("/api/characters/top", methods=["GET"])')
print('def get_top_characters():')
print('    try:')
print('        # Use optimized query with LIMIT and proper indexing')
print('        query = """')
print('            SELECT id, name, crew, bounty, current_price, weekly_change')
print('            FROM characters ')
print('            WHERE is_active = true ')
print('            ORDER BY bounty DESC ')
print('            LIMIT 10')
print('        """')
print('        ')
print('        result = pool.execute(query)')
print('        return jsonify(result[0])')
print('    except Exception as e:')
print('        return jsonify({"error": str(e)}), 500')
print('')
print('🔧 USE PATTERNS FROM THIS MODULE:')
print('- Index strategies (lines 157-193)')
print('- Query optimization (lines 379-404)')
print('- Performance monitoring (lines 447-461)')

print('\n🚀 STEP 4: ADD CONNECTION POOLING TO ALL SERVICES')
print('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━')
print('📝 UPDATE ALL SERVICE FILES:')
print('')
print('# In services/character-service/app.py')
print('import mysql.connector.pooling')
print('')
print('# Create connection pool')
print('pool_config = {')
print('    "pool_name": "character_pool",')
print('    "pool_size": 10,')
print('    "pool_reset_session": True,')
print('    "host": "localhost",')
print('    "database": "onepiece_market",')
print('    "user": "onepiece_user",')
print('    "password": process.env.DB_PASSWORD')
print('}')
print('')
print('pool = mysql.connector.pooling.MySQLConnectionPool(**pool_config)')
print('')
print('# Use pool in queries')
print('def get_character_by_id(character_id):')
print('    connection = pool.get_connection()')
print('    try:')
print('        cursor = connection.cursor(dictionary=True)')
print('        cursor.execute("SELECT * FROM characters WHERE id = %s", (character_id,))')
print('        result = cursor.fetchone()')
print('        return result')
print('    finally:')
print('        connection.close()  # Returns to pool')
print('')
print('🔧 BENEFITS OF CONNECTION POOLING:')
print('- Reduces connection overhead by 90%+')
print('- Handles concurrent users efficiently')
print('- Prevents database connection exhaustion')
print('- Essential for production applications')

print('\n🚀 STEP 5: TEST YOUR DATABASE OPTIMIZATIONS')
print('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━')
print('🧪 TESTING STEPS:')
print('')
print('1. Set up Redis server:')
print('   sudo systemctl start redis-server')
print('   redis-cli ping  # Should return PONG')
print('')
print('2. Test database indexes:')
print('   mysql -u onepiece_user -p onepiece_market')
print('   EXPLAIN SELECT * FROM characters WHERE bounty > 1000000000;')
print('   # Should show index usage')
print('')
print('3. Test Redis caching:')
print('   curl http://localhost:5000/api/characters  # First call (slow)')
print('   curl http://localhost:5000/api/characters  # Second call (fast from cache)')
print('')
print('4. Monitor query performance:')
print('   SHOW PROCESSLIST;  # Check active queries')
print('   SHOW STATUS LIKE "Slow_queries";  # Check slow query count')
print('')
print('5. Test connection pooling:')
print('   # Run multiple concurrent requests')
print('   for i in {1..10}; do curl http://localhost:5001/api/characters & done')
print('')
print('✅ SUCCESS CRITERIA:')
print('- Redis caching reduces response time by 80%+')
print('- Database queries use indexes (shown in EXPLAIN)')
print('- Connection pooling handles concurrent requests')
print('- No slow queries or connection errors')
print('- Character data loads in <100ms')

print('\n===============================================================================')
print('🔗 HOW THIS CONNECTS TO OTHER LEARNING MODULES')
print('===============================================================================')

print('\n🧩 MODULE CONNECTIONS:')
print('')
print('📚 Module 0 (OOP) → Your database models use OOP patterns with SQLAlchemy')
print('📚 Module 16 (Node.js) → API Gateway uses Redis caching and connection pooling')
print('📚 Module 15 (JavaScript) → Async/await patterns for database operations')
print('📚 Module 14 (Django vs SQLAlchemy) → Compare ORM performance optimizations')
print('📚 Module 8 (Monitoring) → Database performance monitoring and alerting')
print('📚 Module 6 (System Design) → Database scaling and sharding strategies')

print('\n🎯 NEXT MODULES TO COMPLETE:')
print('1. Module 16: Implement Redis caching in your API Gateway')
print('2. Module 8: Add database monitoring and alerting')
print('3. Module 6: Design database scaling strategy for growth')

print('\n📚 RECOMMENDED RESOURCES FOR CONTINUED LEARNING:')
print('🔗 MySQL Performance: https://dev.mysql.com/doc/refman/8.0/en/optimization.html')
print('🔗 Redis Best Practices: https://redis.io/docs/manual/patterns/')
print('🔗 Database Indexing: https://use-the-index-luke.com/')
print('🔗 Connection Pooling: https://en.wikipedia.org/wiki/Connection_pool')

print('\n🏴‍☠️ YOU\'RE NOW READY TO BUILD HIGH-PERFORMANCE, SCALABLE DATABASE SYSTEMS! ⚔️')
print('📖 REFERENCE: Check MASTER-BLUEPRINT-ARCHITECTURE.md for the complete system overview!')
